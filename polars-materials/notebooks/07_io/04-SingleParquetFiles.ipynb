{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8093b6a5-2db6-4cde-8cc5-caf21f5cfa6c",
   "metadata": {},
   "source": [
    "# Parquet files 1: Single Parquet files\n",
    "By the end of this lecture you will be able to:\n",
    "- read from a Parquet file\n",
    "- use query optimisation to read a subset of the data\n",
    "- get the schema of a Parquet file\n",
    "- write a Parquet file with compression\n",
    "- write a Parquet file that is larger than memory\n",
    "\n",
    "## What is a Parquet file?\n",
    "A Parquet file is:\n",
    "- a *binary* file where data is ordered in columns rather than rows\n",
    "- each column has a name and a dtype\n",
    "- each column can be compressed separately with automatic dictionary encoding\n",
    "\n",
    "The Apache Parquet and Apache Arrow projects evolved together as columnar formats where Apache Parquet is the format for the data on disk and Apache Arrow is the format for the data in memory.\n",
    "\n",
    "Compared to CSV a Parquet file:\n",
    "- is faster to read and write than a CSV file\n",
    "- takes less space on disk, especially once compression is applied\n",
    "- allows Polars to select which columns to read without parsing the full dataset\n",
    "- preserves the dtypes of columns\n",
    "\n",
    "*I use Parquet files whenever possible for my data engineering pipelines* with the exception of small files that I want to open to read manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f72df-7ddd-42d0-b330-9e077cd5b4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc3985-7fe2-4f68-9ba1-3e1274f9cc0f",
   "metadata": {},
   "source": [
    "## Creating a Titanic Parquet file\n",
    "We begin by creating a Parquet file from the Titanic CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3cf4c-6039-43de-af64-97c5a07c0fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file = \"../data/titanic.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd417d-a431-49d1-9618-3acc8bbd4b90",
   "metadata": {},
   "source": [
    "We create the Parquet Titanic directory in the `data_files/parquet` sub-directory of the `io` sub-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc05e05-4b4a-4924-a1f5-a9f3b4952d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_file_path = Path(\"data_files/parquet/titanic\")\n",
    "if not parquet_file_path.exists():\n",
    "    parquet_file_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca690d-4c94-4d2f-9370-8b4b4a6da113",
   "metadata": {},
   "source": [
    "Now we set the path that we will write the Parquet file to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69aed7b-7a31-4bae-8664-9862ac8f0716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_file = \"data_files/parquet/titanic/titanic.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d2dfc-b56c-4f2c-949c-4838fe2d1b1e",
   "metadata": {},
   "source": [
    "We read the CSV and write to the Parquet path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f58883-e657-42db-af01-c1292c215554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_csv(csv_file).write_parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23cb1ae-2a83-40e0-be75-aae4950b4280",
   "metadata": {},
   "source": [
    "## Reading a Parquet file\n",
    "We read the Parquet file to a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78a2bd-2469-44b6-b77a-42b2944e4b16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pl.read_parquet(parquet_file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f8e01-fcdd-407d-b662-0aeb6246ca0a",
   "metadata": {},
   "source": [
    "As a Parquet file stores the schema as metadata we can get the schema of a Parquet file without having to read any data.\n",
    "\n",
    "In Polars we can use the `read_parquet_schema` function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cce1a-3e53-40bb-8cde-b2cbaf98d4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_parquet_schema(parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ab14c-418e-42e9-81f0-e6c7cfa52d10",
   "metadata": {},
   "source": [
    "We see that the dtypes are preserved in a Parquet file (unlike a CSV file where all data is converted to text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e720d4-5c90-4120-a7cf-f67ec2423bc5",
   "metadata": {},
   "source": [
    "We can select a subset of columns to read from a Parquet file with the `columns` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc75dc2-8d42-4d78-bb1f-4f35e54ecff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquet_file,\n",
    "        columns=[\"Pclass\",\"Name\"]\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fcb744-364c-4884-8371-92f9142b5cbe",
   "metadata": {},
   "source": [
    "When we work in lazy mode in Polars the query optimiser detects when only a subset of columns must be read automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd84b5-c108-4849-8558-078a0da431e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_parquet(parquet_file)\n",
    "    .select([\"Pclass\",\"Name\"])\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510eefdb-01d2-4b9d-9400-0bc7924084b8",
   "metadata": {},
   "source": [
    "We can also specify a smaller number of rows that we want to read with `n_rows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883628bb-19a0-49de-9423-9bec787051ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquet_file,\n",
    "        n_rows=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d0cbb-8ca4-4160-b51e-ccf582e8db1d",
   "metadata": {},
   "source": [
    "If we are running out of memory when reading a Parquet file we can specify `low_memory = True`. This can help to reduce peak memory usage at the expense of a longer load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8776668-910f-4fbd-a0c9-82ffcd80d54e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquet_file,\n",
    "        low_memory=True\n",
    "    )\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8b4f4-5305-4abc-9ddb-88ae04f030d3",
   "metadata": {},
   "source": [
    "Polars reads the Parquet file in multiple threads into different chunks of memory. By default Polars then combines all the chunks into a single chunk in parallel. With the `low_memory=True` argument Polars reduces peak memory usage by not doing this recombination in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39803e28-01e7-401f-a750-f9fdc423eba2",
   "metadata": {},
   "source": [
    "Using `low_memory = True` will not help if the ultimate `DataFrame` does not fit in memory. In this case using `streaming` in lazy mode is the best option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee5c16-2b32-488a-b533-cc94cc8a6e33",
   "metadata": {},
   "source": [
    "## Writing a Parquet file\n",
    "When we write a Parquet file we can specify compression options. I recommend using `zstd` in most cases for a good balance of compressed file size on disk and read time into memory. The `lz4` option is an alternative when faster reading and writing is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1adadc-5e0e-4568-8b67-076729087844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write_parquet(parquet_file,compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ebd84-d099-45dd-bcec-672084696744",
   "metadata": {},
   "source": [
    "We can also adjust the degree of compression with `compression_level`. The range of values depends on the compression scheme chosen - see the docstrings for details.\n",
    "\n",
    "A Parquet file internally is broken into groups of rows (called row groups). Parquet files can store simple statistics of the data in each row group. In a lazy query Polars can use these statistics to determine if only some row groups of the file need to be read.\n",
    "\n",
    "- We can tell Polars to add the statistics for row groups in the file by adding `statistics=True`\n",
    "- We can tell Polars how many rows should be in each row_group with the `row_group_size` (set to 512^2 by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e034c-f674-4ac3-a6f4-a1a43e2672b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet(\n",
    "    parquet_file,\n",
    "    compression=\"zstd\",\n",
    "    statistics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b01ad-79c1-47d2-9165-0dd91533d56c",
   "metadata": {},
   "source": [
    "One further point to get maximum value from the row groups: if you want to query the Parquet file in a certain way (e.g. pull out  date ranges or pull out certain values from a ID column) then sort the `DataFrame` by that column (or columns) before writing to concentrate the data in the smallest number of row groups inside the file.\n",
    "\n",
    "Note: writing statistics can be slow for larger files so it is only worth doing if you will get value from faster repeated queries against that file.\n",
    "\n",
    "If we apply a filter on a **lazy** scan of a Parquet file Polars uses the row groups to reduce how much data must be read - based on on the filter in the `SELECTION` of the query plan below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e394c9-cee0-40dc-a4e7-a1381d4c97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_parquet(parquet_file)\n",
    "    .filter(\n",
    "        pl.col(\"PassengerId\") < 30\n",
    "    )\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01929201-39f7-414b-a195-d15215a11d97",
   "metadata": {},
   "source": [
    "### Writing a larger-than-memory Parquet file\n",
    "We can use streaming to process a larger-than-memory Parquet file (or files) and \"sink\" (or write) the output to another Parquet file. Whereas normal streaming requires the output to fit into memory the `sink` approach removes this contraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb87b5-cbdc-4ce3-af58-0f06ce6353a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sinkparquet_file = \"data_files/parquet/titanic/titanic_sink.parquet\"\n",
    "(\n",
    "    pl.scan_parquet(parquet_file)\n",
    "    .group_by(\"Pclass\")\n",
    "    .agg(\n",
    "        pl.col(\"PassengerId\").count().alias(\"counts\")\n",
    "    )\n",
    "    .sink_parquet(sinkparquet_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6c8ae-00bd-49e4-93d1-07368983e407",
   "metadata": {},
   "source": [
    "The `sink_parquet` approach only requires a `LazyFrame`, the query does not have to begin with a `scan_parquet`. This means we can use it to convert a larger-than-memory CSV file to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6e3fb-65ca-4e6f-9ee3-b72884224d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sinkparquet_file = \"data_files/parquet/titanic/titanic_sink.parquet\"\n",
    "(\n",
    "    pl.scan_csv(csv_file)\n",
    "    .group_by(\"Pclass\")\n",
    "    .agg(\n",
    "        pl.col(\"PassengerId\").count().alias(\"counts\")\n",
    "    )\n",
    "    .sink_parquet(sinkparquet_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5bf43a-0427-41bd-8364-e5de9e47e1ee",
   "metadata": {},
   "source": [
    "There is also a `sink_csv` method available: https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.LazyFrame.sink_csv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f109f59-e856-4764-b133-4e054c96e3cd",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "In the exercises you will develop your understanding of:\n",
    "- read and writing Parquet files\n",
    "- categorical dtypes in Parquet files\n",
    "- reading the schema of Parquet files\n",
    "- reading a subset of Parquet files\n",
    "\n",
    "### Exercise 1\n",
    "We will write a new Parquet file for the exercises to this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f894f-3c42-4484-97b6-0c35fc2ace0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_parquet_file = \"data_files/parquet/titanic/titanic_exercise.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cb840-19fd-4dad-97f4-d8256d5f789a",
   "metadata": {},
   "source": [
    "Before we write to this file read the Parquet file created at the start of the notebook to a `DataFrame`. \n",
    "\n",
    "Convert the `Sex` column to `pl.Categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1b6dd-42fe-429a-b508-8100b6875e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.read_parquet(parquet_file)\n",
    "    .with_columns(<blank>)\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641107b-5c72-4b27-9994-dc6b1f4fe9ab",
   "metadata": {},
   "source": [
    "Write the `DataFrame` with a categorical column to `exercise_parquet_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60f32d-bcd0-46cc-9a10-dca657df0b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29c4c7c-9ea1-4282-8387-4c3c6c1bfdb4",
   "metadata": {},
   "source": [
    "Read the schema of `exercise_parquet_file` to confirm whether Parquet can preserve categorical encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb333b54-9d1f-4cdc-8780-e376ad4fb654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c209952-ea58-48b9-b5c9-82fddb489344",
   "metadata": {},
   "source": [
    "Create a lazy query that only reads these columns\n",
    "```python\n",
    "[\"Survived\",\"Pclass\",\"Age\",\"Sex\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ab6ea-c2aa-43a8-b29c-be62b0fe2810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21dfb0e3-4ce8-497a-a8ea-02d2051b92f2",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Solution to exercise 1\n",
    "We will write a new Parquet file for the exercises to this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2355ad-09ca-4463-b110-ee01c3ecccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_parquet_file = \"data_files/parquet/titanic/titanic_exercise.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43db9a-ea2c-44d8-8808-952fa3237c98",
   "metadata": {},
   "source": [
    "Before we write to this file read the Parquet file created at the start of the notebook to a `DataFrame`. \n",
    "\n",
    "Convert the `Sex` column to `pl.Categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c8a61-e682-47f8-9cc8-0b1b29ee5177",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.read_parquet(parquet_file)\n",
    "    .with_columns(pl.col(\"Sex\").cast(pl.Categorical))\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b9157-2688-4764-9d48-597175eec3d3",
   "metadata": {},
   "source": [
    "Write the `DataFrame` with a categorical column to `exercise_parquet_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501bcf8-305c-4bd9-80ca-e4554b43d347",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write_parquet(exercise_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2f63c-f793-480c-8fc4-43f93588c848",
   "metadata": {},
   "source": [
    "Read the schema of `exercise_parquet_file` to confirm whether Parquet can preserve categorical encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490306b-fd6f-4400-aac4-554b2fdafad8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_parquet_schema(exercise_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5df38-7deb-4e8b-ae26-33763f0ec9e9",
   "metadata": {},
   "source": [
    "Create a lazy query that only reads these columns\n",
    "```python\n",
    "[\"Survived\",\"Pclass\",\"Age\",\"Sex\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298bffa-95e6-4b89-891a-dd81af45bbfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_parquet(exercise_parquet_file)\n",
    "    .select([\"Survived\",\"Pclass\",\"Age\",\"Sex\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c94e65-034c-4cf8-91cb-2e23c9e06331",
   "metadata": {},
   "source": [
    "### Solution to exercise 2\n",
    "This exercise is still in development but you can read through if interested.\n",
    "\n",
    "Here we compare performance reading a Parquet file with and without statistics\n",
    "\n",
    "First create a long `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afd37b-d5cf-4b2f-a57f-51551c555d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 3_000_000\n",
    "df_random = pl.DataFrame(\n",
    "    np.random.standard_normal((N,3)),\n",
    "    schema=[\"a\",\"b\",\"c\"]\n",
    ")\n",
    "df_random.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772af83d-8b00-4cdd-bf86-e1375d909d20",
   "metadata": {},
   "source": [
    "Now we create a new directory to save this Parquet file to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f9dba-63d7-4ecb-a983-1c27afde3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_parquet_file_path = Path(\"data_files/parquet/big\")\n",
    "if not random_parquet_file_path.exists():\n",
    "    random_parquet_file_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc6e0a-0911-49c3-9f62-e535ea43637e",
   "metadata": {},
   "source": [
    "Now we create a file path with and without statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f621c-e703-4872-bb29-f2429f19c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_parquet_stats = random_parquet_file_path / \"stats.parquet\"\n",
    "random_parquet_no_stats = random_parquet_file_path / \"no_stats.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7752e9-c257-4d4d-86ec-8d350a47475b",
   "metadata": {},
   "source": [
    "Save the Parquet file with and without statistics to the appropriate file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3388ba-8349-4837-8af5-b7a251565d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random.write_parquet(random_parquet_stats,statistics=True)\n",
    "df_random.write_parquet(random_parquet_no_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ee2a6-f7b7-4c7c-b3ca-7a9c609c250c",
   "metadata": {},
   "source": [
    "Compare how long the following query needs with and without statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9768ce-66fe-4798-b410-b3e3b490406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r3\n",
    "(\n",
    "    pl.scan_parquet(random_parquet_stats)\n",
    "    .filter(\n",
    "        pl.col(\"a\")<-2\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea1cb8-6793-49fe-992e-4aca7e41b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r3\n",
    "(\n",
    "    pl.scan_parquet(random_parquet_no_stats)\n",
    "    .filter(\n",
    "        pl.col(\"a\")<-2\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2141360-c5ce-4cb0-93ac-184a001724f7",
   "metadata": {},
   "source": [
    "Now try reading again where we have sorted the data by column `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6baab3-7f68-47b1-ae45-338e3fd270e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random.sort(\"a\").write_parquet(random_parquet_stats,statistics=True)\n",
    "df_random.sort(\"a\").write_parquet(random_parquet_no_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02892e36-5454-4ff4-bbdc-4940416fc922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
