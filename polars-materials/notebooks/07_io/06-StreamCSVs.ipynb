{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119da637-425b-4c5a-9c00-45fd1e2c50a4",
   "metadata": {},
   "source": [
    "## CSV files 4: streaming larger-than-memory datasets\n",
    "By the end of this lecture you will be able to:\n",
    "- process larger-than-memory datasets from CSVs with streaming\n",
    "\n",
    "**These concepts also apply to streaming Parquet files**.\n",
    "\n",
    "With streaming Polars can process a full query on a larger-than-memory dataset by:\n",
    "- reading each CSV file in batches\n",
    "- adapting its standard operations to work on batches instead of the full dataset at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cab59-f01d-49f5-81c1-65c0ec6f0d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1e61a-71b9-471e-97ee-b8b4794d3e03",
   "metadata": {},
   "source": [
    "Obviously it doesn't work for me to provide very large datasets with this course. Instead we will do streaming on a small dataset and you can then apply it to your own larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959942c6-7050-487b-93bf-12e6c1bb1eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file = \"../data/nyc_trip_data_1k.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3a6c9-7a84-4f04-9eb6-c3eb32d702a4",
   "metadata": {},
   "source": [
    "We start with a simple non-streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1769b9-1530-4626-9f38-45117a09f615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_csv(csv_file,try_parse_dates = True)\n",
    "    .group_by(\"passenger_count\")\n",
    "    .agg(\n",
    "        pl.col(pl.Float64).mean()\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10010e89-017a-4d7e-8409-3e31ed1315ab",
   "metadata": {},
   "source": [
    "We make this streaming by passing `streaming = True` to `collect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50432929-fc0c-4244-88af-014923c6bac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_csv(csv_file,try_parse_dates = True)\n",
    "    .group_by(\"passenger_count\")\n",
    "    .agg(\n",
    "        pl.col(pl.Float64).mean()\n",
    "    )    \n",
    "    .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dac011-d393-4977-9246-2cba05345cd3",
   "metadata": {},
   "source": [
    "### What happens in streaming mode?\n",
    "In streaming mode for a CSV Polars uses the batched CSV reader that we saw in a previous lecture to read a CSV in batches. However, it goes beyond that to implement the remaining parts of a lazy query in batches as well.\n",
    "\n",
    "Not all operations support streaming - for those that do not Polars uses a non-streaming approach. \n",
    "\n",
    "You can also use the `explain` method to see if a query will use the streaming engine by passing the `streaming=True` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b52a2-7e51-427e-baf3-d76cea77dfaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_csv(csv_file,try_parse_dates = True)\n",
    "    .group_by(\"passenger_count\")\n",
    "    .agg(\n",
    "        pl.col(pl.Float64).mean()\n",
    "    )\n",
    "    .explain(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f250356-3e28-4535-b473-35b3f81bfb88",
   "metadata": {
    "tags": []
   },
   "source": [
    "If the query contains a section with `STREAMING` then it will be executed in streaming mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c48575-6aca-4630-bfa7-39ae40985328",
   "metadata": {},
   "source": [
    "## Controlling batch size\n",
    "Polars uses a simple rule to determine the default size of the batches in streaming mode (see the determine_chunk_size function on this page if you are interested https://github.com/pola-rs/polars/blob/main/crates/polars-pipe/src/pipeline/mod.rs). \n",
    "\n",
    "> The following applies to any streaming query, not just CSVs\n",
    "\n",
    "The rule requires the number of threads Polars can run on. Typically this is set equal to the number of CPU cores on your machine.\n",
    "\n",
    "You can see the number of threads with the `pl.thread_pool_size` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7287371-e2c2-41d6-a8dd-b20fc9d2447a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_threads = pl.thread_pool_size()\n",
    "n_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb047d2-f7a1-4fbf-b5a2-68dfc78c6314",
   "metadata": {},
   "source": [
    "Then Polars sets a variable called `thread_factor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe899840-fc02-4e7c-b075-38583772438a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thread_factor = max(12 / n_threads, 1)\n",
    "thread_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ea4df-99cb-4754-b41d-b00ad0922218",
   "metadata": {},
   "source": [
    "Finally the batch size is set as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17aaf2-8c7b-4674-aafb-4a4425b52af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of cols in the DataFrame/CSV/Parquet\n",
    "n_cols = 10\n",
    "max((50_000 / n_cols) * thread_factor, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409d1e8-fb6c-4ebc-9dda-c58769c54c88",
   "metadata": {},
   "source": [
    "If you want to modify this batch size for your query you can do this with a config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd23f1-b859-4a6b-bfa5-9439117aa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_streaming_chunk_size(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ee858-bf72-478b-a941-1da665bd553e",
   "metadata": {},
   "source": [
    "You can set this parameter at any time and it will affect subsequent queries\n",
    "\n",
    "The number of threads in the threadpool is set equal to the number of CPUs on your machine by default. You can modify this by setting the max threads environment variable **before you import Polars. You need to restart the notebook kernel to see the change in the output of this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f3a89-7c67-45f3-9cec-51beb0e9b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"20\"\n",
    "import polars as pl\n",
    "pl.thread_pool_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328547a-206a-4be0-aedf-16044e6e8fb2",
   "metadata": {},
   "source": [
    "## Streaming joins\n",
    "We can join data from different CSVs with streaming. In this example we join the CITES trade records with the ISO country data on the `Importer` column\n",
    "> See the lectures on joining `DataFrames` if you have not encountered these datasets yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f80eb-bfe9-4dd6-a49b-34738a35d16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "citescsv_file = \"../data/cites_extract.csv\"\n",
    "iso_csv_file = \"../data/countries_extract.csv\"\n",
    "(\n",
    "    pl.scan_csv(citescsv_file)\n",
    "    .join(\n",
    "        pl.scan_csv(iso_csv_file),\n",
    "        left_on=\"Importer\",\n",
    "        right_on=\"alpha-2\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5aa59b-709a-4848-a731-31426bbd111f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Output to a file\n",
    "With standard streaming (i.e. ending a query with `.collect(streaming=True)` Polars needs to output a `DataFrame` from streaming and so the output of the query must fit in memory. \n",
    "\n",
    "However, Polars can write (or `sink`) output directly to a [Parquet](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.LazyFrame.sink_parquet.html#polars.LazyFrame.sink_parquet) or [CSV](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.LazyFrame.sink_csv.html) file in streaming mode - see the Single Parquet lecture for an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70efd6-1dbd-4141-9537-70e15c228acd",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "We can profile a query when we use streaming. \n",
    "\n",
    "> If you have not encountered `profile` see the lecture on Lazy Groupby in the section on Statistics, Counts and Grouping for an introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754084be-59a0-4412-be90-16d535ec43ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groupDf, profileDF = (\n",
    "    pl.scan_csv(csv_file)\n",
    "    .group_by(\"passenger_count\")\n",
    "    .agg(\n",
    "        pl.col(\"trip_distance\").mean()\n",
    "    )\n",
    "    .sort(\"passenger_count\")\n",
    "    .profile(streaming=True,show_plot=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e3be2-52f4-4f79-a08f-bbd5611c0379",
   "metadata": {},
   "source": [
    "The `STREAMING` part of this query captures the parts of the query carried out in the streaming engine. In this case we see that almost the full query was spent in the pipeline while the Polars query optimiser took a very small proportion of the overall time.\n",
    "\n",
    "## Streaming and common subplan elimination\n",
    "The query above produced the following notification from Polars\n",
    "\n",
    "```\n",
    "Cannot combine 'streaming' with 'common_subplan_elimination'. CSE will be turned off.\n",
    "```\n",
    "Common subplan elimination is one of the ways that the query optimiser can optimise queries. It arises in queries where the same action is applied to the same `LazyFrame` in different parts of a query. You generally don't need to worry about seeing this warning.\n",
    "\n",
    "## When does streaming not work?\n",
    "Streaming works for many common operations such as `groupby`,`filter` and `join`. However, there are other common operations where it does not work.\n",
    "\n",
    "One simple example where streaming does not work is the `shift` expression - this replaces the value with the value on the next row. If we look at the query plan below we see that the `shift` is not inside the `STREAMING` part of the query plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757405a-14b0-4bdd-8d7c-67b9f2e70265",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_csv(csv_file)\n",
    "    .with_columns(\n",
    "        pl.col(\"tip_amount\").shift(1)\n",
    "    )\n",
    "    .explain(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6e274-81f3-4669-919e-cab7cad504e3",
   "metadata": {},
   "source": [
    "The reason for this is that streaming works in batches of rows. For the `shift` expression the last row in the batch needs data from another batch. This inter-batch communicaton is not supported and so streaming does not work for `shift`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a593940-c6d5-47db-82e3-e086370b9017",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "There are no exercises here as streaming works in a similar way to operations we have met before.\n",
    "\n",
    "Try streaming on your own data and check if streaming is being used by calling `explain(streaming=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967c9a6-12a4-4c37-9ac9-57149bb10da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
