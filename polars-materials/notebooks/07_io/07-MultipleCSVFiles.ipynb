{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24a78fa-fb7b-4f26-bd78-2883a39f0a89",
   "metadata": {},
   "source": [
    "# Working with multiple files\n",
    "By the end of this lecture you will be able to:\n",
    "- read multiple files with a glob pattern\n",
    "- read multiple files from a list\n",
    "- read multiple files in lazy mode\n",
    "- automate file discovery in sub-directories\n",
    "\n",
    "We import Python's built-in `pathlib` module to work with multiple file paths and create sub-directories.\n",
    "\n",
    "In the example below we use CSV files but the results also apply to Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b939b3a1-683b-4b50-b684-3ab989daf676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "pl.Config.set_tbl_rows(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc655e-ee7e-487b-80d4-fee28463ac3a",
   "metadata": {},
   "source": [
    "We need a dataset with multiple files that share the same schema for this notebook.\n",
    "\n",
    "We create multiple CSV files from the Titanic dataset in a new directory.\n",
    "\n",
    "We begin by reading in the full CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa380c6-464c-44e7-b627-fb373e0d30db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file = \"../data/titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcfbf4-4aea-47a2-af2f-725a274790ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pl.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f47946-e2af-47eb-b2ed-99d6da53677e",
   "metadata": {},
   "source": [
    "We create a new sub-directory in this directory.\n",
    "\n",
    "We use the `mkdir` method of a `Path` object to create this new sub-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023c963-e064-403f-affe-7717b4c70cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the new directory\n",
    "csv_directory = Path(\"data_files/csv/multiple_csv\")\n",
    "# Create the new directory if it doesn't already exist\n",
    "csv_directory.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1c0e3-1dc4-4018-b894-2d5563a5b98a",
   "metadata": {},
   "source": [
    "We split the `DataFrame` and write the two new files to the sub-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d9173-0b40-4ec1-863c-ccb059144e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[:700].write_csv(csv_directory / \"train.csv\")\n",
    "df[700:].write_csv(csv_directory / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa575804-c0ab-4630-b11e-7bc6f3c0b6cd",
   "metadata": {},
   "source": [
    "## Eager mode\n",
    "### Reading multiple files with wildcard patterns\n",
    "\n",
    "We can read multiple CSV files with the same schema using a wildcard `*` pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa006e3-b8e7-4791-a33f-6220d7129bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(csv_directory / \"*.csv\")\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52cb23-9f1f-4add-81d2-28397585ee60",
   "metadata": {},
   "source": [
    "The files are read in alphabetical order where `test` comes before `train`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d6ba4-5e06-4364-8416-7598dc50ac82",
   "metadata": {},
   "source": [
    "#### What happens when we use the wildcard pattern `*`?\n",
    "When we use the wildcard pattern `*` as above Polars internally:\n",
    "- make a list of the files that match the pattern\n",
    "- calls `scan_csv` on each file to make a list of `LazyFrames`\n",
    "- does a vertical concatenation of the `LazyFrames`\n",
    "- calls `collect` to return a `DataFrame`\n",
    "\n",
    "Essentially using `read_csv` with `*` is an automated version of the lazy mode approach we see below.\n",
    "\n",
    "### What happens if there is a potential optimisation?\n",
    "In the query below we do `read_csv` followed by a `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe449d-d94e-4390-9f29-0a52f592f5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(\"data_files/csv/multiple_csv/*.csv\")\n",
    "    .filter(pl.col(\"Pclass\") == 1)\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc1e592-82b5-4c88-97c3-e963c047ce62",
   "metadata": {},
   "source": [
    "Although Polars uses `pl.scan_csv` internally the overall query is eager and the query optimiser is not used in this query. This means that if we follow `read_csv` with - for example - a `filter` method then each CSV is read in full into memory, concatenated into a single `DataFrame` and then the `filter` is applied.\n",
    "\n",
    "### Reading from a list of file paths\n",
    "\n",
    "If we have a list of file paths we can also read them manually with `pl.concat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50f536-9fa6-4a94-bd08-acefeb8ca1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_list = [csv_directory / \"train.csv\",csv_directory / \"test.csv\"]\n",
    "(\n",
    "    pl.concat(\n",
    "        [pl.read_csv(csv_path) for csv_path in file_path_list]\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fb917-edd7-49d1-971c-0dc62de8cefa",
   "metadata": {},
   "source": [
    "## Scanning CSVs in lazy mode\n",
    "\n",
    "### Scanning multiple files with a wildcard\n",
    "We can scan multiple CSV files in a directory in lazy mode using a wildcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7befb1c-231a-4efd-a9a4-48449986edfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_csv(csv_directory / \"*.csv\")\n",
    "    .filter(pl.col(\"Age\") > 50)\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3cb226-6b8c-47db-9662-573b8a454bf9",
   "metadata": {},
   "source": [
    "The plan shows us that Polars:\n",
    "- creates a plan for each file e.g. `PLAN 0`\n",
    "- applies the `filter` on each file`\n",
    "- concatenates the output from each plan to a single `DataFrame` in `UNION`\n",
    "\n",
    "Unlike the eager query above the query optmiser is working here.\n",
    "\n",
    "We evaluate this plan on all the CSVs with `collect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe14efc-1493-43ba-b3e1-ad3038dba9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_csv(csv_directory / \"*.csv\")\n",
    "    .filter(pl.col(\"Age\") > 50)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf0d72-fd16-4e2f-b924-a452498c0479",
   "metadata": {},
   "source": [
    "## Handling variations in column names\n",
    "We cannot concatenate CSVs that have different column names with `pl.scan_csv`\n",
    "\n",
    "In this example we create write two `DataFrames` with slightly different column names to a new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f29bf-150c-4dc8-95b4-2cbe226e54ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = pl.DataFrame(\n",
    "    {\n",
    "        'int_column':[0,1,2]\n",
    "    }\n",
    ")\n",
    "df2 = pl.DataFrame(\n",
    "    {\n",
    "        'Int_Column':[3,4]\n",
    "    }\n",
    ")\n",
    "# Create a sub-directory to hold the CSV for each DataFrame\n",
    "mismatched_column_names_path = Path('data_files/csv/mismatched_column_names/')\n",
    "if not mismatched_column_names_path.exists():\n",
    "    mismatched_column_names_path.mkdir()\n",
    "# Write the DataFrames to a CSV\n",
    "df1.write_csv(mismatched_column_names_path / 'df1.csv')\n",
    "df2.write_csv(mismatched_column_names_path / 'df2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9d8f2-1396-4201-822a-ec0b4a20d7b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we try to call `pl.scan_csv` with a `*` we get an `Exception` (commented out to allow my automated checks to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee4661-4315-4bed-adbf-1c1c43a2eb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (\n",
    "#     pl.scan_csv(mismatched_column_names_path / 'df*.csv')\n",
    "#     .collect()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32adc21-3e58-4cc8-a62c-f564fa9e00d8",
   "metadata": {},
   "source": [
    "We handle this using the `with_column_names` argument to modify the column names before we concatenate the data from different files.\n",
    "\n",
    "In this example we specify a function to casts the column names to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb35e77-47e6-43c1-851d-512884051131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_csv(\n",
    "        mismatched_column_names_path / 'df*.csv',\n",
    "        with_column_names=lambda cols: [col.lower() for col in cols]\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce3e50-26f2-4edf-9367-81c70c961b26",
   "metadata": {},
   "source": [
    "### Scanning from a list of file paths in lazy mode\n",
    "We can also create a list of scanned CSV files in lazy mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80613e6f-5d54-4184-acf4-b8732fa3ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = [\n",
    "    'data_files/csv/multiple_csv/train.csv',\n",
    "    'data_files/csv/multiple_csv/test.csv'\n",
    "]\n",
    "queries_list = [\n",
    "    pl.scan_csv(csv_path) for csv_path in files_list\n",
    "]\n",
    "queries_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51e070-924a-485f-84cf-754a2e22de0c",
   "metadata": {},
   "source": [
    "The `queries_list` is a `list` of `LazyFrames`.\n",
    "\n",
    "Polars can evaluate a `list` of `LazyFrames` with `pl.collect_all`.  The output is a `list` of `DataFrames`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2725870-8d05-4c32-832c-f3528440b02f",
   "metadata": {},
   "source": [
    "To return the output as a single `DataFrame` we call:\n",
    "- `pl.concat` to combine the `list` of `LazyFrames` to a single `LazyFrame`\n",
    "- `collect` to evaluate the `LazyFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad2956-fac8-4109-baf0-d0e47a7ca7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.concat(\n",
    "        queries_list\n",
    "    )\n",
    "    .collect()\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2164a46-2474-42a4-979d-ac83547c9fd5",
   "metadata": {},
   "source": [
    "For large datasets we can use streaming with `streaming = True` in `collect`.\n",
    "\n",
    "If the column names are in different orders or there are small differences in the dtypes (e.g. floats in one file and integers in another) we can reconcile these by concatenating with the `vertical_relaxed` method as show in the Concatenation lecture. \n",
    "\n",
    "## Discovering file paths\n",
    "In some cases we want an easy way to find all the CSVs in sub-directories.\n",
    "\n",
    "We can use PyArrow in this case. While using PyArrow isn't necessary in this simple example, it is handy with more complicated directory structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ac785-feb7-4f60-b852-faeddb6a09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "dataset = ds.dataset(\n",
    "    csv_directory,\n",
    "    format=\"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0674e9-f801-4d33-8e24-7c006d9042b4",
   "metadata": {},
   "source": [
    "We list the files that PyArrow has found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80efdd0f-b9e4-461c-933d-ae46630763f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd0da5-5dc6-459b-89ee-05367e751f98",
   "metadata": {},
   "source": [
    "We can then read these files in eager mode by:\n",
    "- letting PyArrow turn them into an Arrow table and\n",
    "- creating a Polars `DataFrame` from the Arrow table with zero-copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829732c-1c61-49df-a1fd-4ca94caeef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.from_arrow(\n",
    "        dataset.to_table()\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e01a0e0-8ddd-47e2-bf88-11c255582c16",
   "metadata": {},
   "source": [
    "With PyArrow we can do manual optimisations such as limit the columns or apply a row filter in the arguments of `to_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e9e47-869d-4482-a521-294fe62e0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.from_arrow(\n",
    "        dataset.to_table(\n",
    "            columns=[\"Pclass\",\"Age\"],\n",
    "            filter = ds.field(\"Age\") > 70)\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6aa378-5356-4925-885e-854375e1255a",
   "metadata": {},
   "source": [
    "See the PyArrow docs for more info on the `dataset` object: https://arrow.apache.org/docs/python/dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2518604-4790-4eb2-8172-3fd0d6a288cc",
   "metadata": {},
   "source": [
    "## So which approach should you use?\n",
    "Each of these approaches will work, but these are my opinions for general cases:\n",
    "- If you want to read all files into memory with no query optimisations use `pl.read_csv`\n",
    "- Use a wildcard if you can specify the files using a wildcard\n",
    "- Use a list if you want more control over which files you read\n",
    "- Use PyArrow if you have a more complicated directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf36629-d239-476a-96a4-ab64bb64fe61",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "In the exercises you will develop your understanding of:\n",
    "- reading multiple CSV files in eager mode\n",
    "- reading multiple CSV files in lazy mode\n",
    "- reading CSVs with PyArrow\n",
    "\n",
    "### Exercise 1\n",
    "The NYC taxi dataset CSV has 1000 rows containing records from different days.\n",
    "\n",
    "### Set-up\n",
    "We transform this CSV into a set of partitioned CSVs in sub-directories. \n",
    "\n",
    "We first set the path to the full CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb08d2-e9bd-459c-a069-abb138d03670",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyccsv_file = \"../data/nyc_trip_data_1k.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da09fe-bdb1-464d-941a-733ef7a987ed",
   "metadata": {},
   "source": [
    "We now:\n",
    "- read the CSV\n",
    "- add a column that records the date from the `pickup` datetime\n",
    "- partition the `DataFrame` into a dictionary that maps dates to the `DataFrame` for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22981b-1e28-4110-af6c-0c54076edb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDfDict = (\n",
    "    pl.read_csv(nyccsv_file,try_parse_dates=True)\n",
    "    .with_columns(\n",
    "    pl.col(\"pickup\").dt.truncate(\"1d\").dt.strftime(\"%Y-%m-%d\").alias(\"pickup_day\")\n",
    "    )\n",
    "    .partition_by(by=[\"pickup_day\"],as_dict=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb985d-08da-439d-bfcb-f1e02b81d2c8",
   "metadata": {},
   "source": [
    "The keys of the `dailyDfDict` are the string dates for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1049dff-5323-49a2-97d8-50051ab78cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDfDict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84340d4-c23a-4a3d-ac57-0cc49297b0f4",
   "metadata": {},
   "source": [
    "The values for each key is a `DataFrame` for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32757ce9-a002-42d4-a6cb-96f6bf20ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDfDict['2022-01-01',].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1e13e-55ae-4401-9eff-a00899abad9b",
   "metadata": {},
   "source": [
    "We now create a partitioned directory called `daily_nyc` for the data.\n",
    "\n",
    "The name of each sub-directory is a date.\n",
    "\n",
    "The content of each sub-directory is the CSV for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b3134-e608-451e-9506-27f8209f39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new directory\n",
    "nyccsv_directory = Path(\"data_files/csv/daily_nyc\")\n",
    "\n",
    "# Create the new directory if it doesn't already exist\n",
    "nyccsv_directory.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# Loop through each date\n",
    "for (day,), df in dailyDfDict.items():\n",
    "    # Create a Path object for that date\n",
    "    dailyDirectory = (nyccsv_directory / day)\n",
    "    # Create the sub-directory for that date\n",
    "    dailyDirectory.mkdir(parents=True,exist_ok=True)\n",
    "    # Write a CSV called daily.csv\n",
    "    df.write_csv(dailyDirectory / \"daily.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27b57f-d0aa-466a-9008-7446a138a196",
   "metadata": {},
   "source": [
    "We list the contents of `daily_nyc` to see the sub-directories for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37428579-5150-4c69-a7dc-046d23817058",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data_files/csv/daily_nyc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6feadc7-c6d4-4031-82cd-4675ef6ffe2d",
   "metadata": {},
   "source": [
    "We list the contents of one sub-directory to show the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2841ef-7f3d-4531-a03c-72f26514f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data_files/csv/daily_nyc/2022-01-01/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2dcb7-acf0-449c-8a96-b859357a3834",
   "metadata": {},
   "source": [
    "### Now on to the exercise!\n",
    "\n",
    "Read all the CSV files in eager mode using a path with wildcards for the final directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f8b7f-4974-4520-a1cb-21c0ba5ebfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(\n",
    "        \"data_files/csv/daily_nyc<blank>\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2df8f-4ed7-4b14-991e-8b8807ce3867",
   "metadata": {},
   "source": [
    "Read the CSV files in eager mode using:\n",
    "- a `glob` and a `generator`\n",
    "- a concatenation of the list of `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68962c4d-9b9f-4c9f-a4ff-93fcd25edf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycfile_paths_generator = nyccsv_directory<blank>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba8a1e-5706-4b0d-91cb-e176c5d99127",
   "metadata": {},
   "source": [
    "Read all the CSV files in lazy mode using a path with wildcards for the final directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdaff2f-3222-489e-b4c8-5c0e9a18459f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30725747-3765-41d4-a69f-7982ef4ba9d3",
   "metadata": {},
   "source": [
    "Read all the CSVs in lazy mode **between 2022-01-01 and 2022-01-09** inclusive\n",
    "\n",
    "- Scan the required `DataFrames` by iterating through the generator\n",
    "- Call `collect_all` to evaluate all the `LazyFrames`\n",
    "- `concat` all the `DataFrames`\n",
    "\n",
    "If you want a hint about filtering the dates expand the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507db69-c36e-463e-a89d-342813ad79a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hint: in an `if` statement convert the `csv_path` to string with `csv_path.as_posix()` and check if 2022-01-0\n",
    "# is in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018551e6-b09e-41cd-b9d6-cb44b886f296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7b7557-abda-48c2-b37d-0a426dfe3d6b",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Create a PyArrow `dataset` object with all the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30751d20-e153-4756-8b12-a683ba8035fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b112bc7e-3da6-4f33-8882-77b845a1fa2f",
   "metadata": {},
   "source": [
    "List all the CSV files in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373439fe-82d6-447c-b3c9-aed93dfa4ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c9097d7-c087-4504-ab84-4df184f5457d",
   "metadata": {},
   "source": [
    "Read all the files into a Polars `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c8b51-5cc7-47f6-bb18-15815cb4a9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f65a5073-474c-4f7c-9af7-1af067b90e71",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Solution to exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca15b09-1b6f-41ad-8f89-1bdb7fcf2d6f",
   "metadata": {},
   "source": [
    "Read all the CSV files in eager mode using a path with wildcards for the final directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606bc16-7137-463a-a6bc-17bbc3d415da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_csv(\"data_files/csv/daily_nyc/*/daily.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa5876-af30-4e29-a91a-5418715819fa",
   "metadata": {},
   "source": [
    "Read the CSV files in eager mode using:\n",
    "- a `glob` and a `generator`\n",
    "- a concatenation of the list of `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66739a-eaa5-4932-af48-1afb80160e1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_paths_generator = nyccsv_directory.glob(\"*/*.csv\")\n",
    "(\n",
    "    pl.concat(\n",
    "        [pl.read_csv(csv_path) for csv_path in file_paths_generator]\n",
    "    )\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1119bf9-51aa-445d-b5c0-91b9bf911fa5",
   "metadata": {},
   "source": [
    "Read all the CSV files in lazy mode using a path with wildcards for the final directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7647b52-9047-4eea-89be-f143800157dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_csv(\"data_files/csv/daily_nyc/*/daily.csv\")\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60392580-06e2-4a7e-95d6-4e446c528a96",
   "metadata": {},
   "source": [
    "Read all the CSVs in lazy mode *between 2022-01-01 and 2022-01-09** inclusive\n",
    "\n",
    "- Scan the required `DataFrames` by iterating through the generator\n",
    "- Call `collect_all` to evaluate all the `LazyFrames`\n",
    "- `concat` all the `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08dfe5-83a3-4197-8c2d-1c222502dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint: in an `if` statement convert the `csv_path` to string with `csv_path.as_posix()` and check if 2022-01-0\n",
    "# is in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165117c-de0f-4224-ba09-f5bec0f81ee6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nycfile_paths_generator = nyccsv_directory.glob(\"*/daily.csv\")\n",
    "(\n",
    "    pl.concat(\n",
    "        pl.collect_all(\n",
    "            [pl.scan_csv(csv_path) for csv_path in nycfile_paths_generator if \"2022-01-0\" in csv_path.as_posix()]\n",
    "        )\n",
    "    )\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b9673-f0fb-4c62-9d32-e3db404cceeb",
   "metadata": {},
   "source": [
    "### Solution to exercise 2\n",
    "Create a PyArrow `dataset` object with all the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606fda8-3ecc-4129-85e4-5a1156d875a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ds.dataset(nyccsv_directory,format=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eca454-eac9-44d1-854d-ad724d2132c8",
   "metadata": {},
   "source": [
    "List all the CSV files in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f47f7-971a-4ef6-9bb0-598688e3181e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301b5db-eb8d-4f7f-a8ec-23e8116a0152",
   "metadata": {},
   "source": [
    "Read all the files into a Polars `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193abab-2fff-4adf-8c3b-326ddc9f1de6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.from_arrow(\n",
    "        dataset.to_table()\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
