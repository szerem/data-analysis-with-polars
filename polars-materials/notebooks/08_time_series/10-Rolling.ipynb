{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc25ef89-0488-4a10-b307-7ddd8ac81c11",
   "metadata": {},
   "source": [
    "## Rolling time series analysis\n",
    "By the end of this lecture you will be able to:\n",
    "- calculate rolling time series expressions\n",
    "- do rolling groupbys\n",
    "- identify the use cases of `rolling` and `group_by_dynamic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a62be1-482f-4eec-a391-85317c2f1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "pl.Config.set_tbl_rows(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee5c0f-e9ce-4134-abfe-96ddaa77a29e",
   "metadata": {},
   "source": [
    "We first create a simple time series `DataFrame` with hourly data over one day. We add a `values` column that tracks the row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473eb376-3d5e-446e-a565-63634a19212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = datetime(2020,1,1)\n",
    "end_datetime = datetime(2020,1,2)\n",
    "df = (\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"date\":pl.datetime_range(\n",
    "                start_datetime,\n",
    "                end_datetime,\n",
    "                interval=\"1h\",\n",
    "                eager=True\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    .with_row_index(\"values\")\n",
    "    # Re-order the columns to have date first\n",
    "    .select(\"date\",\"values\")\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b544b7-a020-4175-b3c8-24af65af9630",
   "metadata": {},
   "source": [
    "### Built-in rolling aggregations\n",
    "Polars has built-in expressions to do a rolling-groupby and aggregations for common aggregations such as mean,min, max, sum. \n",
    "\n",
    "However, the syntax is as follows:\n",
    "```python\n",
    "pl.col(\"values\").rolling_mean(\n",
    "            by=\"date\",\n",
    "            window_size=\"3h\",\n",
    "            closed=\"right\"\n",
    "        )\n",
    "```\n",
    "where:\n",
    "- we first create the expression that will be aggregated in each rolling window with `pl.col(\"values\")`\n",
    "- we call `rolling_mean` to tell polars we want to create rolling windows and take the mean of `pl.col(\"values\")` in each window\n",
    "- we specify how the windows are defined with the arguments to `rolling_mean`\n",
    "\n",
    "The rolling expression groups rows into *windows*. We can specify these length of these windows using a time period using a python `timedelta` or a Polars interval string. \n",
    "\n",
    "\n",
    "In this example we calculate the rolling mean and sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b7bf0-7c31-4871-abab-1e5e29b08488",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .with_columns(\n",
    "        roll_mean = pl.col(\"values\").rolling_mean(\n",
    "            by=\"date\",\n",
    "            window_size=\"3h\",\n",
    "            closed=\"right\"\n",
    "        ),\n",
    "        roll_sum = pl.col(\"values\").rolling_sum(\n",
    "            by=\"date\",\n",
    "            window_size=timedelta(hours=3),\n",
    "            closed=\"right\"\n",
    "        ),\n",
    "\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d9fc9-28da-4d13-a4ee-bd3de900a7e6",
   "metadata": {},
   "source": [
    "It is important to understand how rolling windows are defined. For the `rolling_*` expressions the windows are defined as:\n",
    "\n",
    "(t_0 - window_size, t_0]\n",
    "\n",
    "(t_1 - window_size, t_1)]\n",
    "\n",
    "â€¦\n",
    "\n",
    "(t_n - window_size, t_n]\n",
    "\n",
    "where `t_0` is the first data point, `t_1` is the second data point etc.. This means:\n",
    "- the first window with right closure is rows with values from `21:00` (not included) to `00:00` (included) so just the `00:00` row\n",
    "- the second window is rows with values from `22:00` (not included) to `01:00` (included) so the `00:00` and `01:00` rows\n",
    "- the last row is the rows from `21:00` (not included) to `00:00` (included) so the `22:00`,`23:00` and `00:00` rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55469ac6-b850-4707-8a3c-1fdd5a484ac5",
   "metadata": {},
   "source": [
    "## Rolling expression\n",
    "The `rolling_*` expressions above are only available for the most common aggregations. We can specify any expression to be evaluated in rolling windows using the `rolling` expression.\n",
    "\n",
    "The syntax of a sample `rolling` expression is slightly different from the `rolling_*` expressions above:\n",
    "```python\n",
    "(\n",
    "    pl.col(\"values\")\n",
    "    .mean()\n",
    "    .rolling(\n",
    "        index_column=\"date\",\n",
    "        period=\"3h\"\n",
    "    )\n",
    ")    \n",
    "```\n",
    "This means:\n",
    "- create rolling windows based on the `date` column with a window size of 3 hours and\n",
    "- in each window take the mean of the `values` column\n",
    "\n",
    "Here we use `index_column` instead of `by` and `period` instead of `window_size`.\n",
    "\n",
    "In this example we use the `first` and `last` expressions to help us understand how the rolling windows are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc594440-a2b2-4bbd-b2c8-8960c3431c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .with_columns(\n",
    "        # Get the first row index in each window\n",
    "        window_row_first = pl.col(\"values\").first().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\"\n",
    "        ),\n",
    "        # Get the last row index in each window\n",
    "        window_row_last = pl.col(\"values\").last().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\"\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43873115-eaf1-4441-a9e2-ea1c5087ca7a",
   "metadata": {},
   "source": [
    "The windows are closed on the right by default and are defined as (t_0 - period, t_0] so:\n",
    "- the first window is from `21:00:00` to `00:00:00` and so only includes `00:00:00`\n",
    "- the second window is from `22:00:00` to `01:00:00` and so includes `00:00:00` and `01:00:00`\n",
    "- the third window is from `23:00:00` to `02:00:00` so includes `00:00:00`, `01:00:00` and `02:00:00`\n",
    "- the fourth window is from `00:00:00` to `03:00:00` but excludes `00:00:00` as it is open on the left\n",
    "\n",
    "We can offset the start of the windows with `offset`. The default offset is `-period` or `-3h`. In this example we shift all windows two hours later than this default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0fcdb6-1708-4ae0-81de-f463b0631ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .with_columns(\n",
    "        window_row_first = pl.col(\"values\").first().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\",\n",
    "            offset=\"-1h\",\n",
    "        ),\n",
    "        window_row_last = pl.col(\"values\").last().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\",\n",
    "            offset=\"-1h\",\n",
    "        ),\n",
    "        window_row_indexes = pl.col(\"values\").agg_groups().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\",\n",
    "            offset=\"-1h\",\n",
    "        ),\n",
    "\n",
    "    )\n",
    "    .head(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84095493-78fa-40b8-82d4-7168d530ee84",
   "metadata": {},
   "source": [
    "With the first window now starts 2 hours later and runs from `23:00:00` to `02:00:00`.\n",
    "\n",
    "We can do aggregations on each rolling window by using an expression. In this example we do the rolling `mean` and `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d663cb6-c58e-4d8f-820d-92a1b858ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .with_columns(\n",
    "        roll_mean = pl.col(\"values\").mean().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\",\n",
    "        ),\n",
    "        roll_sum = pl.col(\"values\").sum().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\",\n",
    "        ),\n",
    "    )\n",
    "    .head(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3090b-6d6c-4073-a3ac-ba18797a0e8d",
   "metadata": {},
   "source": [
    "## Rolling on a `DataFrame`\n",
    "The `rolling_*` or `rolling` expressions are useful if we want to add a new column or columns to a `DataFrame`.\n",
    "\n",
    "If instead we want to create a new `DataFrame` with all rolling columns we can also call `rolling` on a `DataFrame`.\n",
    "\n",
    "This approach is preferable if:\n",
    "- we want to create a whole `DataFrame` of rolling values or\n",
    "- we have numerous rolling calculations and we want to ensure we only calculate the rolling windows once\n",
    "\n",
    "The syntax is the same as for the `rolling` expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c256a95-dea0-408d-95bf-811a1ba797f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\",\n",
    "        )\n",
    "    .agg(\n",
    "        window_row_first = pl.col(\"values\").first(),\n",
    "        window_row_last = pl.col(\"values\").last()\n",
    "        \n",
    "    )\n",
    "    .head(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b461637-d2f6-4e8f-9717-b8150e7349a7",
   "metadata": {},
   "source": [
    "### Rolling windows by group\n",
    "We may want to do the rolling operation by group such as getting the rolling returns on different stocks or products. To illustrate this we create a new `DataFrame` which is two copies of the original `DataFrame` with an `id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13398e37-7828-421e-be60-c5f4e070e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            # Copy with value A in the id column\n",
    "            df.with_columns(id = pl.lit(\"A\")),\n",
    "            # Copy with value B in the id column\n",
    "            df.with_columns(id = pl.lit(\"B\")),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "df2                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbadc8-452c-4634-a4bb-8bb28936c356",
   "metadata": {},
   "source": [
    "If we want to do the rolling operations by groups we pass the `by` argument to specify the groups. \n",
    "\n",
    "In this example Polars: \n",
    "- first does a `group_by` on the `id` column to make a `DataFrame` for each group\n",
    "- does the `rolling-agg` on each group's `DataFrame`\n",
    "- concatenates the results for each group into a single `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dbbb59-acff-4a56-9f66-f3a2de2d3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df2\n",
    "    .rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\",\n",
    "            by=\"id\"\n",
    "        )\n",
    "    .agg(\n",
    "        window_row_first = pl.col(\"values\").first(),\n",
    "        window_row_last = pl.col(\"values\").last()\n",
    "        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e7b62-aaec-4f22-8eb4-9096175248c0",
   "metadata": {},
   "source": [
    "Note that for the rolling calculations to be correct the data must be sorted by the `index_column` within each group.\n",
    "\n",
    "## Lazy mode and streaming?\n",
    "We can use the `rolling` expressions and groupby on a `LazyFrame`.\n",
    "\n",
    "Unfortunately rolling operations are not currently supported by the streaming engine. \n",
    "\n",
    "We can see that rolling operations are not streaming in the query plan below as the `WITH_COLUMNS` below is above the `--- STREAMING` block of the query plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f04b73-648d-497a-a694-25a7f646a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    df\n",
    "    .lazy()\n",
    "    .with_columns(\n",
    "        window_row_first = pl.col(\"values\").first().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\"\n",
    "        ),\n",
    "        window_row_last = pl.col(\"values\").last().rolling(\n",
    "            index_column=\"date\",\n",
    "            period=\"3h\"\n",
    "        ),\n",
    "    )\n",
    "    .explain(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef0a05-ede1-42e0-a7ef-ca615c4ec48d",
   "metadata": {},
   "source": [
    "So why is rolling not supported by the streaming engine?\n",
    "\n",
    "Recall that the streaming engine works in batches of rows and is best suited for operations that can be worked on in batches independently. However, some rolling windows will start in one batch and end in another meaning they cannot be worked on in batches independently. Therefore rolling does not currently work in streaming mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1ebf5-6c79-42b9-9964-2a8221cd4704",
   "metadata": {},
   "source": [
    "## Rolling or `group_by_dynamic`?\n",
    "The `rolling` and `group_by_dynamic` methods both do windowed time series aggregations but there is a difference between them:\n",
    "- `group_by_dynamic` works with constant window intervals\n",
    "- `rolling` works with windows that depend on the data\n",
    "\n",
    "Essentially `group_by_dynamic` looks at your first and last time points and divides up the time interval into boxes. But `rolling` instead looks at each time point and creates a window around each one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a712941-da12-458f-8ee1-41e83b39395d",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises\n",
    "In the exercises you will learn how to:\n",
    "- do rolling statistics\n",
    "- manage rolling windows\n",
    "\n",
    "### Exercise 1\n",
    "First we set some helpful config settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783610a7-f134-4c28-8d47-0f3af79b78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_fmt_str_lengths(100)\n",
    "pl.Config.set_fmt_float(\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc3c31-8a49-4aa9-8909-90be49e9a500",
   "metadata": {},
   "source": [
    "We create a `DataFrame` from the Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939374b-68ff-4fd2-bf32-8aac61eab9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_csv = \"../data/spotify-charts-2017-2021-global-top200.csv.gz\"\n",
    "spotify_df = pl.read_csv(spotify_csv,try_parse_dates=True)\n",
    "spotify_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb9979-80db-4aa1-b547-fa7020ba76ef",
   "metadata": {},
   "source": [
    "For the track `Starboy` add a one-week `rolling_mean` and `rolling_max` of the streams column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2650f4-394e-4b91-bfbd-9d377234ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spotify_df\n",
    "    .filter(pl.col(\"title\")==\"Starboy\")\n",
    "    .select(\"title\",\"artist\",\"date\",\"trend\",\"streams\")\n",
    "    <blank>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df5ae9-d499-4c2a-b7cd-60ae296fe988",
   "metadata": {},
   "source": [
    "We are going to visualise the rolling mean number of streams for the most popular tracks\n",
    "\n",
    "- Add a column called `title_artist` that is a string concatenation of the `title` and `artist` columns separated by `:` (see here if you are not familar with the function to do this: https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.concat_str.html#polars-concat-str)\n",
    "- Sort the `DataFrame` by the `title_artist` and `date` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42150d2a-3d08-4ace-a783-8d5fe5a7c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_spotify_df = (\n",
    "    spotify_df\n",
    "    <blank>\n",
    ")\n",
    "roll_spotify_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7075ba2-21f3-4bec-8f25-d806afae9472",
   "metadata": {},
   "source": [
    "Continue by:\n",
    "- doing a weekly `rolling` groupby on the `date` column by `title_artist`\n",
    "- creating an aggregated `roll_streams` column with the weekly mean of the streams for each track\n",
    "- sorting the output by `roll_streams` with the largest values at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155aa598-7449-46f7-9a2c-cb919cbd875e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "827b805c-5196-4b4f-85d7-47e1a7cd2e27",
   "metadata": {},
   "source": [
    "The next step is more advanced, check out the hints below or the solution if you are not sure where to start!\n",
    "\n",
    "We want to continue by visualising the results only for the most popular tracks. However, we want to keep all dates for these tracks, not just the most streamed dates so:\n",
    "\n",
    "- filter the `DataFrame` to keep **all** rows for any track that appears in the top 300 rows of `roll_streams`\n",
    "\n",
    "The output should have 14,358 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deabe86-6e97-4db2-b569-38370568429f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Hint1:\n",
    "# Continue by using a pipe function on the output from the query above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f9952-737a-4b17-90ca-376e806ab109",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Hint2\n",
    "# In the pipe function do a semi-join with the rows that have the most popular tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed72af-a6e3-4c68-9c13-2af083298bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c162f74-a205-4d4a-b980-7f65ac76a84b",
   "metadata": {},
   "source": [
    "Visualise the results as a time series line chart with Plotly (or your preferred plotting library) with\n",
    "- time on the x-axis\n",
    "- `roll_streams` on the y-axis\n",
    "- `title_artist` in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2b6e5-41f7-4796-b761-f4be3712a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.line(\n",
    "    <blank>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4e45d-94bd-45ef-b581-3dd8c8d7fcac",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Solution to exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1c333-b766-4e53-9aab-ec5ae44f7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_fmt_str_lengths(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168d936-2170-4c23-9711-f57f0bc63c36",
   "metadata": {},
   "source": [
    "We create a `DataFrame` from the Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0b5bd-551c-4c25-90fd-e86ec935d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_csv = \"../data/spotify-charts-2017-2021-global-top200.csv.gz\"\n",
    "spotify_df = pl.read_csv(spotify_csv,try_parse_dates=True)\n",
    "spotify_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17246e3-4d99-4d81-be84-9e5733235d65",
   "metadata": {},
   "source": [
    "For the track `Starboy` add a one-week `rolling_mean` and `rolling_max` of the streams column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da45ba-7d80-48e7-81d5-da0add906544",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    spotify_df\n",
    "    .filter(pl.col(\"title\")==\"Starboy\")\n",
    "    .select(\"title\",\"artist\",\"date\",\"trend\",\"streams\")\n",
    "    .sort(\"date\")\n",
    "    .with_columns(\n",
    "        roll_mean = pl.col(\"streams\").rolling_mean(\n",
    "            by=\"date\",\n",
    "            window_size=\"1w\",\n",
    "            closed=\"right\"\n",
    "        ),\n",
    "        roll_max = pl.col(\"streams\").rolling_max(\n",
    "            by=\"date\",\n",
    "            window_size=\"1w\",\n",
    "            closed=\"right\"\n",
    "        )\n",
    "\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0185084-2dae-4971-995e-d529e759faaa",
   "metadata": {},
   "source": [
    "We are going to visualise the rolling mean number of streams for the most popular tracks\n",
    "\n",
    "- Add a column called `title_artist` that is a string concatenation of the `title` and `artist` columns separated by `:` (see here if you are not familar with the function to do this: https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.concat_str.html#polars-concat-str)\n",
    "- Sort the `DataFrame` by the `title_artist` and `date` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92364ad-3a89-4376-ab18-5bcb354df5e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "roll_spotify_df = (\n",
    "    spotify_df\n",
    "    .with_columns(pl.concat_str([\"title\",\"artist\"],separator=\":\").alias(\"title_artist\"))\n",
    "    .sort(\"title_artist\",\"date\")\n",
    ")\n",
    "roll_spotify_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbe98a-8b38-4006-85d3-7b6eb4cdff7b",
   "metadata": {},
   "source": [
    "Continue by:\n",
    "- doing a weekly `rolling` groupby on the `date` column by `title_artist`\n",
    "- creating an aggregated `roll_streams` column with the weekly mean of the streams for each track\n",
    "- sorting the output by `roll_streams` with the largest values at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e54ae8-7564-44b3-a40b-35202d99e54b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "roll_spotify_df = (\n",
    "    spotify_df\n",
    "    .with_columns(pl.concat_str([\"title\",\"artist\"],separator=\":\").alias(\"title_artist\"))\n",
    "    .sort(\"title_artist\",\"date\")\n",
    "    .rolling(\n",
    "        index_column=\"date\",\n",
    "        period=\"1d\",\n",
    "        by=\"title_artist\"\n",
    "        )\n",
    "    .agg(\n",
    "        roll_streams = pl.col(\"streams\").mean()\n",
    "    )\n",
    "    .sort(\"roll_streams\",descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec0bdf-df7f-4f67-a87f-8061fe64e778",
   "metadata": {},
   "source": [
    "The next step is more advanced, check out the solution if you are not sure where to start!\n",
    "\n",
    "We want to visualise the results only for the most popular tracks. However, we want to keep all dates for these tracks, not just the most streamed dates so:\n",
    "\n",
    "- filter the `DataFrame` to keep all rows for any tracks that appear in the top 500 rows of `roll_streams`\n",
    "\n",
    "The output should have 14,358 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc056a3-b4b8-4329-8250-16e36f8373bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint1:\n",
    "# Use a pipe function on the output from the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb659426-b45c-47b0-9736-02a74d43233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint2\n",
    "# In the pipe function do a semi-join with the output from the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66eb38-a222-44f0-9da7-f4fcab97af9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "roll_spotify_df = (\n",
    "    spotify_df\n",
    "    .with_columns(pl.concat_str([\"title\",\"artist\"],separator=\":\").alias(\"title_artist\"))\n",
    "    .sort(\"title_artist\",\"date\")\n",
    "    .rolling(\n",
    "        index_column=\"date\",\n",
    "        period=\"1mo\",\n",
    "        by=\"title_artist\"\n",
    "        )\n",
    "    .agg(\n",
    "        roll_streams = pl.col(\"streams\").mean()\n",
    "    )\n",
    "    .sort(\"roll_streams\",descending=True)\n",
    "    .pipe(\n",
    "        lambda df:df.join(\n",
    "            # Get the first 500 rows and filter the dataframe for these title-artist combinations\n",
    "            df.head(300),on=\"title_artist\",how=\"semi\")\n",
    "    )\n",
    "    .sort(\"date\")\n",
    ")\n",
    "roll_spotify_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e6c3f-fd4f-4537-8965-e50d5455d6cd",
   "metadata": {},
   "source": [
    "Visualise the results as a time series line chart with Plotly (or your preferred plotting library) with\n",
    "- time on the x-axis\n",
    "- `roll_streams` on the y-axis\n",
    "- `title_artist` in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098a1fe-7cd1-48fe-bc14-5520c8a21333",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.line(\n",
    "    roll_spotify_df,\n",
    "    x=\"date\",\n",
    "    y=\"roll_streams\",\n",
    "    color=\"title_artist\",\n",
    "    width=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf5734-ac1b-403b-b6a8-ac0d3f0b2000",
   "metadata": {},
   "source": [
    "See how the streams for some tracks grew slowly while others started at high values!\n",
    "\n",
    "Vary the rolling period to 1 day and 1 month to see the smoothing effect of the rolling analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdafcbea-2d7f-4832-bb95-b63d09bd931e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
